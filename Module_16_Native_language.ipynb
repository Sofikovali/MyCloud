{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с естественным текстом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В прошлых блоках мы сравнивали числа и строки между собой самым простым образом: они либо в точности совпадали, либо нет. При работе с реальными текстами это доставит нам много проблем, т. к. в большинстве случаев слова пишутся в разных падежах и склонениях. Например, чтобы понять, близки ли описания товаров на сайте между собой, уже недостаточно просто найти общие слова. \n",
    "\n",
    "В этом модуле мы научимся:\n",
    "\n",
    "Получать основу и словарную форму слова (процедура стемминга и лемматизации). Это позволит принципиально улучшить любые сравнения и вычисления, связанные с анализом текстов. Например, поиск дубликатов описаний товаров.\n",
    "Строить статистику уникальных слов в наборе поисковых запросов с учетом падежей и склонений. При этом мы будем исключать из результатов так называемые стоп-слова (союзы, предлоги и другие лишние слова)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Основные библиотеки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом блоке мы рассмотрим примеры работы трех библиотек для решения так называемых задач обработки естественного языка, а также решим задачу подсчета слов в наборе поисковых запросов с учетов падежей и склонений.\n",
    "\n",
    "Библиотека Pymystem (https://pypi.python.org/pypi/pymystem3) считается одной из лучших для преобразования слов в словарную форму. Разработана в Яндексе, умеет работать с несуществующими словами. Недостаток — самая медленная из рассматриваемых."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymystem3\n",
      "  Downloading https://files.pythonhosted.org/packages/00/8c/98b43c5822620458704e187a1666616c1e21a846ede8ffda493aabe11207/pymystem3-0.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.6/site-packages (from pymystem3) (2.18.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in ./anaconda3/lib/python3.6/site-packages (from requests->pymystem3) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in ./anaconda3/lib/python3.6/site-packages (from requests->pymystem3) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in ./anaconda3/lib/python3.6/site-packages (from requests->pymystem3) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.6/site-packages (from requests->pymystem3) (2019.3.9)\n",
      "Installing collected packages: pymystem3\n",
      "Successfully installed pymystem3-0.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing mystem to /Users/kovaleva/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-macosx.tar.gz\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем работать со статистикой 100 тысяч поисковых запросов, состоящих из двух столбцов: самого поискового запроса и количества его появлений. Выведем на экран первые 10 строк файла, разбитых по табуляции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['вк', '1']\n",
      "['одноклассники', '1']\n",
      "['порно', '1']\n",
      "['ютуб', '1']\n",
      "['вконтакте', '1']\n",
      "['одноклассники моя страница', '3']\n",
      "['майл', '1']\n",
      "['авито', '1']\n",
      "['переводчик', '1']\n",
      "['яндекс', '1']\n",
      "['сбербанк онлайн', '2']\n",
      "['mail', '1']\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "with open( 'keywords.txt', 'r', encoding = 'utf-8' ) as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split('\\t')\n",
    "        print( line )\n",
    "        if i > 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем значения из первого столбца в переменную word, второго — в wordCount:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "вк 1\n",
      "одноклассники 1\n",
      "порно 1\n",
      "ютуб 1\n",
      "вконтакте 1\n",
      "одноклассники моя страница 3\n",
      "майл 1\n",
      "авито 1\n",
      "переводчик 1\n",
      "яндекс 1\n",
      "сбербанк онлайн 2\n",
      "mail 1\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "with open( 'keywords.txt', 'r', encoding = 'utf-8' ) as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split('\\t')\n",
    "        word = line[0]\n",
    "        wordCount = line[1]\n",
    "        print( word, wordCount )\n",
    "        if i > 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь для каждого слова word мы можем получить его словарную форму (т. е. лемму). Выведем слово и его словарную форму на экран. Это займет некоторое время, т. к. Pymystem работает не очень быстро:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "вк вк\n",
      "\n",
      "одноклассники одноклассник\n",
      "\n",
      "порно порно\n",
      "\n",
      "ютуб ютуб\n",
      "\n",
      "вконтакте вконтакте\n",
      "\n",
      "одноклассники моя страница одноклассник мой страница\n",
      "\n",
      "майл майл\n",
      "\n",
      "авито авито\n",
      "\n",
      "переводчик переводчик\n",
      "\n",
      "яндекс яндекс\n",
      "\n",
      "сбербанк онлайн сбербанк онлайн\n",
      "\n",
      "mail mail\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "with open( 'keywords.txt', 'r', encoding = 'utf-8' ) as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split('\\t')\n",
    "        word = line[0]\n",
    "        wordCount = line[1]\n",
    "        lemmas = m.lemmatize( word )\n",
    "        print( word, ''.join(lemmas) )\n",
    "        if i > 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что некоторые слова изменились в своей словарной форме. Теперь, когда мы будем считать суммарную статистику этих слов в файле, мы получим все варианты написания слова как одно, т. е. его словарную форму."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получите лемму строчки 'и хрюкотали зелюки как мюмзики в мове'. При выводе результата используйте конструкцию print( ''.join( ... ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "и хрюкотали зелюки как мюмзики в мове и хрюкотать зелюк как мюмзик в мов\n",
      "\n"
     ]
    }
   ],
   "source": [
    "line = 'и хрюкотали зелюки как мюмзики в мове'\n",
    "line = line.strip().split('\\t')\n",
    "for i in line:\n",
    "    lemmas = m.lemmatize( i )\n",
    "    print( i, ''.join(lemmas) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK (Natural Language Toolkit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для более быстрой обработки текста рассмотрим библиотеку NLTK (Natural Language Toolkit), которая по умолчанию уже установлена в Anaconda. Библиотека умеет работать со многими языками, а также имеет встроенный список стоп-слов, который мы используем в ближайшей задаче. Работает быстрее Pymystem и проводит процедуру стемминга, т. е. процесс нахождения основы слова.\n",
    "\n",
    "Давайте выведем на экран аналогичные пары слов из нашего файла, но с использованием NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "вк вк\n",
      "одноклассники одноклассник\n",
      "порно порн\n",
      "ютуб ютуб\n",
      "вконтакте вконтакт\n",
      "одноклассники моя страница одноклассники моя страниц\n",
      "майл майл\n",
      "авито авит\n",
      "переводчик переводчик\n",
      "яндекс яндекс\n",
      "сбербанк онлайн сбербанк онлайн\n",
      "mail ma\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer( \"russian\" )\n",
    "i = 0\n",
    "with open( 'keywords.txt', 'r', encoding = 'utf-8' ) as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split('\\t')        \n",
    "        word = line[0]\n",
    "        wordCount = line[1]      \n",
    "        stem = snowball_stemmer.stem( word )\n",
    "        print( word, ''.join( stem ) )        \n",
    "        if i > 10:\n",
    "            break        \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процесс происходит гораздо быстрее, т. к. стемминг проще лемматизации. Разница между этими методами не всегда очевидна, поэтому часто результаты оказываются одинаковыми.\n",
    "\n",
    "Мы используем библиотеку NLTK в обработке всего файла в этом блоке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотека Pymorphy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, рассмотрим самый «продвинутый» вариант разбора слов — морфологический анализатор pymorphy.https://pymorphy2.readthedocs.io/en/latest/index.html Процесс установки аналогичен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 635kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting docopt>=0.6 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
      "Collecting dawg-python>=0.7 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
      "Collecting pymorphy2-dicts<3.0,>=2.4 (from pymorphy2)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.1MB 1.2MB/s ta 0:00:011    50% |████████████████▎               | 3.6MB 2.7MB/s eta 0:00:02\n",
      "\u001b[?25hBuilding wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/kovaleva/Library/Caches/pip/wheels/9b/04/dd/7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\n",
      "Successfully built docopt\n",
      "Installing collected packages: docopt, dawg-python, pymorphy2-dicts, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем библиотеку в наш код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим принцип ее работы на примере морфологического анализа слова 'стали':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='стали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='стать', score=0.984662, methods_stack=((<DictionaryAnalyzer>, 'стали', 904, 4),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 1),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,datv'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 2),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,loct'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 5),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,nomn'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 6),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,accs'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 9),))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse( 'стали' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека производит анализ слова и выдает его возможные атрибуты (обратите внимание, что на выходе получился лист из нескольких элементов Parse). Например, первый элемент с параметром score=0.984662 указывает на то, что слово 'стали', скорее всего, является глаголом (от слова 'стать'). Тэги в элементе tag указывают на так называемые граммемы, характеризующие данное слово: множественное или единственное число, настоящее или прошедшее время и т. д. Остальные элементы результата указывают, что слово 'стали' может быть существительным (от слова 'сталь').\n",
    "\n",
    "Применим аналогичный разбор к первым 5 словам нашего файла:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parse(word='вк', tag=OpencorporaTag('UNKN'), normal_form='вк', score=1.0, methods_stack=((<UnknAnalyzer>, 'вк'),))]\n",
      "[Parse(word='одноклассники', tag=OpencorporaTag('NOUN,anim,masc plur,nomn'), normal_form='одноклассник', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'одноклассники', 2, 6),))]\n",
      "[Parse(word='порно', tag=OpencorporaTag('NOUN,inan,neut,Fixd sing,nomn'), normal_form='порно', score=0.08333333333333333, methods_stack=((<DictionaryAnalyzer>, 'порно', 23, 0),)), Parse(word='порно', tag=OpencorporaTag('NOUN,inan,neut,Fixd sing,gent'), normal_form='порно', score=0.08333333333333333, methods_stack=((<DictionaryAnalyzer>, 'порно', 23, 1),)), Parse(word='порно', tag=OpencorporaTag('NOUN,inan,neut,Fixd sing,datv'), normal_form='порно', score=0.08333333333333333, methods_stack=((<DictionaryAnalyzer>, 'порно', 23, 2),)), Parse(word='порно', tag=OpencorporaTag('NOUN,inan,neut,Fixd sing,accs'), normal_form='порно', score=0.08333333333333333, methods_stack=((<DictionaryAnalyzer>, 'порно', 23, 3),)), Parse(word='порно', tag=OpencorporaTag('NOUN,inan,neut,Fixd sing,ablt'), normal_form='порно', score=0.08333333333333333, methods_stack=((<DictionaryAnalyzer>, 'порно', 23, 4),)), Parse(word='порно', tag=OpencorporaTag('NOUN,inan,neut,Fixd sing,loct'), normal_form='порно', score=0.08333333333333333, methods_stack=((<DictionaryAnalyzer>, 'порно', 23, 5),)), Parse(word='порно', tag=OpencorporaTag('NOUN,inan,neut,Fixd plur,nomn'), normal_form='порно', score=0.08333333333333333, methods_stack=((<DictionaryAnalyzer>, 'порно', 23, 6),)), Parse(word='порно', tag=OpencorporaTag('NOUN,inan,neut,Fixd plur,gent'), normal_form='порно', score=0.08333333333333333, methods_stack=((<DictionaryAnalyzer>, 'порно', 23, 7),)), Parse(word='порно', tag=OpencorporaTag('NOUN,inan,neut,Fixd plur,datv'), normal_form='порно', score=0.08333333333333333, methods_stack=((<DictionaryAnalyzer>, 'порно', 23, 8),)), Parse(word='порно', tag=OpencorporaTag('NOUN,inan,neut,Fixd plur,accs'), normal_form='порно', score=0.08333333333333333, methods_stack=((<DictionaryAnalyzer>, 'порно', 23, 9),)), Parse(word='порно', tag=OpencorporaTag('NOUN,inan,neut,Fixd plur,ablt'), normal_form='порно', score=0.08333333333333333, methods_stack=((<DictionaryAnalyzer>, 'порно', 23, 10),)), Parse(word='порно', tag=OpencorporaTag('NOUN,inan,neut,Fixd plur,loct'), normal_form='порно', score=0.08333333333333333, methods_stack=((<DictionaryAnalyzer>, 'порно', 23, 11),))]\n",
      "[Parse(word='ютуб', tag=OpencorporaTag('NOUN,inan,femn plur,gent'), normal_form='ютуба', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'туб', 55, 8), (<UnknownPrefixAnalyzer>, 'ю')))]\n",
      "[Parse(word='вконтакте', tag=OpencorporaTag('NOUN,inan,masc,Fixd,Sgtm sing,nomn'), normal_form='вконтакте', score=0.16666666666666666, methods_stack=((<DictionaryAnalyzer>, 'вконтакте', 1957, 0),)), Parse(word='вконтакте', tag=OpencorporaTag('NOUN,inan,masc,Sgtm,Fixd sing,gent'), normal_form='вконтакте', score=0.16666666666666666, methods_stack=((<DictionaryAnalyzer>, 'вконтакте', 1957, 1),)), Parse(word='вконтакте', tag=OpencorporaTag('NOUN,inan,masc,Sgtm,Fixd sing,datv'), normal_form='вконтакте', score=0.16666666666666666, methods_stack=((<DictionaryAnalyzer>, 'вконтакте', 1957, 2),)), Parse(word='вконтакте', tag=OpencorporaTag('NOUN,inan,masc,Fixd,Sgtm sing,accs'), normal_form='вконтакте', score=0.16666666666666666, methods_stack=((<DictionaryAnalyzer>, 'вконтакте', 1957, 3),)), Parse(word='вконтакте', tag=OpencorporaTag('NOUN,inan,masc,Fixd,Sgtm sing,ablt'), normal_form='вконтакте', score=0.16666666666666666, methods_stack=((<DictionaryAnalyzer>, 'вконтакте', 1957, 4),)), Parse(word='вконтакте', tag=OpencorporaTag('NOUN,inan,masc,Fixd,Sgtm sing,loct'), normal_form='вконтакте', score=0.16666666666666666, methods_stack=((<DictionaryAnalyzer>, 'вконтакте', 1957, 5),))]\n",
      "[Parse(word='одноклассники моя страница', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='одноклассники моя страница', score=1.0, methods_stack=((<FakeDictionary>, 'классники моя страница', 141, 0), (<KnownSuffixAnalyzer>, 'аница'), (<KnownPrefixAnalyzer>, 'одно')))]\n",
      "[Parse(word='майл', tag=OpencorporaTag('NOUN,anim,femn,Name plur,gent'), normal_form='майла', score=0.33999999999999997, methods_stack=((<DictionaryAnalyzer>, 'айл', 68, 8), (<UnknownPrefixAnalyzer>, 'м'))), Parse(word='майл', tag=OpencorporaTag('NOUN,anim,femn,Name plur,accs'), normal_form='майла', score=0.33999999999999997, methods_stack=((<DictionaryAnalyzer>, 'айл', 68, 10), (<UnknownPrefixAnalyzer>, 'м'))), Parse(word='майл', tag=OpencorporaTag('NOUN,inan,masc sing,nomn'), normal_form='майл', score=0.15999999999999998, methods_stack=((<FakeDictionary>, 'майл', 33, 0), (<KnownSuffixAnalyzer>, 'айл'))), Parse(word='майл', tag=OpencorporaTag('NOUN,inan,masc sing,accs'), normal_form='майл', score=0.15999999999999998, methods_stack=((<FakeDictionary>, 'майл', 33, 3), (<KnownSuffixAnalyzer>, 'айл')))]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "with open( 'keywords.txt', 'r', encoding = 'utf-8' ) as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split('\\t')      \n",
    "        word = line[0]\n",
    "        wordCount = line[1]       \n",
    "        morph_analyze = morph.parse( word )\n",
    "        print( morph_analyze )       \n",
    "        if i > 5:\n",
    "            break       \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "вк вк\n",
      "одноклассники одноклассник\n",
      "порно порно\n",
      "ютуб ютуба\n",
      "вконтакте вконтакте\n",
      "одноклассники моя страница одноклассники моя страница\n",
      "майл майла\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "with open( 'keywords.txt', 'r', encoding = 'utf-8' ) as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split('\\t')       \n",
    "        word = line[0]\n",
    "        wordCount = line[1]        \n",
    "        morph_analyze = morph.parse( word )\n",
    "        print( word, morph_analyze[0].normal_form )        \n",
    "        if i > 5:\n",
    "            break        \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, мы изучили еще один способ приведения слов к единой форме."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дан текст:\n",
    "\n",
    "words = 'Крымский отель Mriya Resort & Spa признали лучшим в мире курортным комплексом для отдыха по версии престижной международной премии World Travel Awards'\n",
    "\n",
    "Необходимо вывести на экран существительные в этом тексте.\n",
    "\n",
    "Подсказка. После получения разбора слова (можно сделать по аналогии с кодом в занятии в переменной morph_analyze) проверить результат на \"признак\" существительного можно с помощью кода:\n",
    "\n",
    "if 'NOUN' in morph_analyze[0].tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = 'Крымский отель Mriya Resort & Spa признали лучшим в мире курортным комплексом для отдыха по версии престижной международной премии World Travel Awards'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "отель отель\n",
      "мире мир\n",
      "комплексом комплекс\n",
      "отдыха отдых\n",
      "версии версия\n",
      "премии премия\n"
     ]
    }
   ],
   "source": [
    "line = words.strip().split()       \n",
    "for i in line:      \n",
    "    a = morph.parse(i)    \n",
    "    if 'NOUN' in a[0].tag:\n",
    "        print( i, a[0].normal_form )        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регулярные выражения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень важным типом фильтров являются так называемые регулярные выражения. В этом модуле мы ненадолго отойдем от построения рекомендательной системы, чтобы разобраться в работе регулярных выражений на нескольких примерах. Такого рода задачи будут часто встречаться вам при работе с данными любого типа:\n",
    "\n",
    "Как найти номера банковских карт в списке транзакций.\n",
    "Отфильтровать список URL страниц по шаблону '/ и восемь цифр подряд'.\n",
    "Проверка корректности email-адресов.\n",
    "Подсчет статистики почтовых систем и выделение логинов пользователей из email-адресов.\n",
    "Фильтры, которые мы рассматривали в прошлых блоках, были относительно простыми: сравнения чисел, поиск подстроки. Более сложные условия и проверки задавали с помощью внешних функций. Однако часто встречаются задачи, в которых необходимо внести дополнительные условия прямо в условие фильтра. Например, найти все слова, которые состоят из 16 цифр (т. е. нужно найти все номера банковских карт в тексте).\n",
    "\n",
    "Для решения таких задач предназначены регулярные выражения. Это правила, которые позволяют задавать шаблоны поиска по строке (например, 16 цифр подряд). Если этот шаблон найден в строке, то она удовлетворяет регулярному выражению.\n",
    "\n",
    "Многие системы аналитики и языки программирования имеют встроенные возможности по использованию регулярных выражений. Стоит помнить, что в каждой системе эти правила могут немного отличаться. Составлено большое число таблиц и сервисов, которые облегчают работу с регулярными выражениями. Например:\n",
    "\n",
    "    Проверка регулярных выражений в питоне онлайн pyregex.com.\n",
    "    Короткая памятка по регулярным выражениям petefreitag.com/cheatsheets/regex/.\n",
    "    Длинная памятка rexegg.com/regex-quickstart.html.\n",
    "    \n",
    "Допустим, нужно выделить из следующего текста номера карт из 16 цифр:\n",
    "\n",
    "card number #1 1234123412341234\n",
    "wrong number 9876\n",
    "card number #1 4321432143214321\n",
    "Согласно правилам регулярных выражений, шаблон для 16 цифр будет выглядеть как «любая цифра, повторенная 16 раз». Что соответствует такой записи: \\d{16}. Убедимся, что онлайн-проверка дает верный результат, т. е. находит номера 1234123412341234 и 4321432143214321 и исключает все остальные варианты. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сложный фильтр для URL страниц"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте рассмотрим, как делать то же самое в своем коде на примере следующей задачи. Имеется набор URL страниц проекта (файл URLs.txt), нам необходимо выделить из этого списка страницы новостей, которые содержат восьмизначный номер новости. Т. е. нам нужны страницы вида '/world/36007585-tramp-pridumal-kak-reshit-ukrainskiy-vopros/?smi2=1', а другие страницы вроде '/politics/', '/latest/?page=1' и '/' нас не интересуют.\n",
    "\n",
    "Такие задачи часто встречаются, когда в системах аналитики (например, Google Analytics) вам нужно получить статистику определенного раздела. Например, сколько просмотров страниц было у новостных текстов за период? С помощью регулярных выражений можно решить подобную задачу с помощью фильтра в одну строку.\n",
    "\n",
    "Импортируем стандартную библиотеку для работы с регулярными выражениями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь необходимо задать шаблон (т. е. правило для фильтра) для страниц вида /world/36007585-tramp-pridumal-kak-reshit-ukrainskiy-vopros/?smi2=1. Сформулируем его так: «любая последовательность символов, затем /, затем 8 цифр, затем тире и последовательность символов». Распишем эту последовательность в терминах регулярных выражений:\n",
    "\n",
    ".* — это любая последовательность символов (точка означает любой символ, * — повторение прошлого символа любое количество раз). Итого .* — это последовательность любых символов (нулевой длины, кстати, тоже);\n",
    "/ прямой слэш;\n",
    "[0-9] любая цифра от 0 до 9 (кстати таким же способом можно задавать любую букву алфавита: [a-z])\n",
    "{8} количество повторений прошлого символа, т. е. в нашем случае цифры;\n",
    "— тире;\n",
    ".* снова любое количество символов.\n",
    "Запишем наше выражение в шаблон:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = '.*/[0-9]{8}-.+'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переведем наш шаблон в объект регулярного выражения. Это существенно ускорит поиск при обработке больших файлов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prog = re.compile(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пройдемся по файлу URL.txt. Для простоты пока не используем Pandas, а просто выведем его содержимое на экран:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'URLs.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-53edf2b9eebd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'URLs.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'URLs.txt'"
     ]
    }
   ],
   "source": [
    "with open('URLs.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()        \n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавляем проверку на соответствие регулярному выражению:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('URLs.txt', 'r') as f:    \n",
    "    for line in f:\n",
    "        line = line.strip()        \n",
    "        # если текст строки удовлетворяем регулярному выражению pattern, то выводим строку\n",
    "        if prog.match( line ):\n",
    "            print( line )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка корректности email-адреса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регулярные выражения позволяют не только фильтровать данные, но и извлекать полезные данные. В этом шаге мы извлечем название домена из списка email-адресов и построим распределение почтовых сервисов этой базы, причем одновременно будем проверять корректность адреса.\n",
    "\n",
    "В этом шаге мы будем работать с файлом 'email_base.csv', в котором записаны 1000 email-адресов в зашифрованном виде. Причем часть строк имеют «битые» адреса без домена. Нам необходимо для каждого адреса определить, является ли он валидным, и выделить имя пользователя и домен.\n",
    "\n",
    "Файл email_base.csv\n",
    "\n",
    "Запишем файл в датафрейм emails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "emails = pd.read_csv('email_base.csv', sep = '\\t', names = ['email'])\n",
    "emails.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформулируем правило корректного email-адреса в нашей задаче следующим образом: «последовательность символов, точек или тире, затем @, затем последовательность символов и на конце '.ru' или '.com'». Реальные проверки для валидации корректности адреса более сложные, но в данном случае мы можем ограничиться такой.\n",
    "\n",
    "Запишем сначала шаблон такой проверки только для адресов домена .ru:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = '[\\w\\.-]+@[\\w]+\\.ru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# адрес должен соответствовать шаблону\n",
    "if re.match(pattern, 'username@yandex.ru'):\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# адрес должен соответствовать шаблону\n",
    "if re.match(pattern, 'username-1990@yandex.ru'):\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# адрес не должен соответствовать шаблону\n",
    "if re.match(pattern, '@yandex.ru'):\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# адрес не должен соответствовать шаблону\n",
    "if re.match(pattern, 'username@yandex'):\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, чтобы добавить проверку доменов .com, допишем правило с комбинацией ru и com через знак логического ИЛИ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = '[\\w\\.-]+@[\\w]+(\\.ru|\\.com$)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# адрес должен соответствовать шаблону\n",
    "if re.match(pattern, 'username-1990@gmail.com'):\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# адрес должен соответствовать шаблону\n",
    "if re.match(pattern, 'username-1990@gmail.com123'):\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Статистика почтовых систем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, наш шаблон проверки работает. Давайте решим следующую задачу: необходимо посчитать количество адресов в файле email_base.csv в разбивке по почтовым доменам. Т. е. посчитать сколько адресов принадлежат yandex.ru, сколько - gmail.com итд. А если адрес не похож на email, то необходимо посчитать количество таких строк как 'wrong email'\n",
    "\n",
    "Для решения этой задачи сначала выделим имя пользователя и домен. Для этого укажем эти email-адреса части в скобках шаблона регулярного выражения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = '([\\w\\.-]+)@([\\w]+(\\.ru|\\.com)*)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В поиске нам поможет метод search. Перебор результатов можно делать с помощью метода group, в качестве аргумента пишем номер группы в скобках:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'username'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# то что стоит в первых скобках\n",
    "re.search(pattern, 'username@yandex.ru').group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yandex.ru'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# то что стоит во вторых скобках\n",
    "re.search(pattern, 'username@yandex.ru').group(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.ru'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# то что стоит во внутренних скобках\n",
    "re.search(pattern, 'username@yandex.ru').group(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь оформим проверку соответствия регулярному выражению в функцию. Если адрес не подходит под шаблон, то ставим 'wrong email':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_email_domain(row):\n",
    "    if re.match(pattern, row['email']):\n",
    "        return re.search(pattern, row['email']).group(2)  \n",
    "    else:\n",
    "        return 'wrong email'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применяем функцию к датафрейму email и пишем результат в столбец 'domain':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emails' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-4b9e1a2864f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memails\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'domain'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_email_domain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0memails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'emails' is not defined"
     ]
    }
   ],
   "source": [
    "emails['domain'] = emails.apply(get_email_domain, axis = 1)\n",
    "emails.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можем получить распределение почтовых систем для нашей базы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emails' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b2bb68541001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0memails\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'domain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'emails' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "emails['domain'].value_counts().plot.bar(color = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails.loc[emails['domain'] == 'wrong email']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выделение email-адресов из текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регулярные выражения дают еще одну крайне полезную возможность выделить все шаблоны в тексте. Давайте получим из следующего текста адреса электронной почты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'Андрей Марков страхование markov_chains@yandex.ru. Мария Кюри технологии mary_decay@gmail.com Петр Капица онлайн-образование study-hard@rambler.ru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = '([\\w\\.-]+)@([\\w]+(\\.ru|\\.com))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('markov_chains', 'yandex.ru', '.ru'),\n",
       " ('mary_decay', 'gmail.com', '.com'),\n",
       " ('study-hard', 'rambler.ru', '.ru')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили набор групп group шаблона pattern. Давайте изменим порядок скобок, чтобы не разделять имя пользователя и домен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = '([\\w\\.-]+@[\\w]+(\\.ru|\\.com))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('markov_chains@yandex.ru', '.ru'),\n",
       " ('mary_decay@gmail.com', '.com'),\n",
       " ('study-hard@rambler.ru', '.ru')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь в первом элементе каждой строки выводится искомый email-адрес. Запишем итоговый алгоритм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "markov_chains@yandex.ru\n",
      "mary_decay@gmail.com\n",
      "study-hard@rambler.ru\n"
     ]
    }
   ],
   "source": [
    "for address in re.findall(pattern, text):\n",
    "    print(address[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Постановка задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, давайте применим новые знания для решения следующей задачи: имеется список отзывов о ресторанах. Необходимо разделить их на положительные и отрицательные. В случае невозможности классификации ставить 'undef'.\n",
    "\n",
    "Файл с отзывами texts_opinions.txt\n",
    "\n",
    "Файл с отзывами имеет кодировку UTF-8 и может некорректно отображаться при открытии в браузере.\n",
    "\n",
    "Для классификации для каждого отзыва будем использовать следующий алгоритм:\n",
    "\n",
    "Предварительно составляем список основ слов, которые характеризуют положительные и отрицательные отзывы. Используем SnowballStemmer библиотеки NLTK (можно было использовать Pymystem, сейчас для простоты используем NLTK).\n",
    "Разбиваем текст отзыва на отдельные слова (здесь нам заранее придется удалить все знаки препинания).\n",
    "Заменяем каждое слово на его основу с помощью SnowballStemmer библиотеки NLTK.\n",
    "Ищем основу каждого слова отзыва среди основ слов из пункта 1. Считаем каких слов получилось больше - из списка «положительных» или «отрицательных» слов. Если таких слов в отзыве не нашлось или их число совпало, то возвращаем 'undef'.\n",
    "После классификации отзывов мы можем сверить наш результат с «правильной» классификацией texts_ratings.txt, которую выставляли сами пользователи, когда писали эти отзывы.\n",
    "\n",
    "Файл texts_ratings.txt\n",
    "\n",
    "Несколько замечаний:\n",
    "\n",
    "Сейчас мы не рассматриваем качество и правдивость составленных отзывов. Наша задача — научиться пользоваться инструментами для подобных задач.\n",
    "Существует множество алгоритмов решения этой задачи, в том числе с помощью машинного обучения. Абсолютное большинство из них начинаются так же, как и наш: со стемминга или лемматизации исходных текстов.\n",
    "Аналогичный подход может сильно помочь вам в задачах фильтрации данных по словам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, сначала составим список основ слов, которые характерны для положительных и отрицательных отзывов. Чтобы не учитывать их многочисленные формы используем стеммер NLTK. Например, найдем основу слова «благодарны»:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'благодарн'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer( \"russian\" )\n",
    "snowball_stemmer.stem( 'благодарны' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичную основу будут иметь многие вариации этого слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "благодарн\n",
      "благодарн\n",
      "благодарн\n"
     ]
    }
   ],
   "source": [
    "for word in ['благодарность', 'благодарностью', 'благодарны']:\n",
    "    print(snowball_stemmer.stem( word ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы взяли несколько простых вариантов «положительных» и «отрицательных» наборов слов, чтобы получить самый простой классификатор. Запишем их в файл params.yaml.\n",
    "Теперь напишем алгоритм классификатора. Сначала импортируем список слов в переменную params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import load\n",
    "params = load( open('params.yaml', mode = 'r', encoding = 'utf-8') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала необходимо удалить из текста знаки препинания, чтобы его можно легко было разбить на слова через пробел. Воспользуемся методом translate. Метод берет список знаков препинания symbols и применяет к ним метод translate, заменяя их на пробелы. Для этого мы заводим строку spaces из такого же количества пробелов, что и symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear_punctuation(text):\n",
    "    \"\"\"Удаление знаком пунктуации из текста text\"\"\"\n",
    "    symbols = '.,!()\"<>'\n",
    "    spaces = ' ' * len(symbols)\n",
    "    return text.translate( text.maketrans(symbols, spaces) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Просто шикарный клуб  Ходили с другом на  Animal Джаz   Остались очень довольны  атмосфера очень уютная  дружелюбная  есть второй этаж  бар'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clear_punctuation('Просто шикарный клуб! Ходили с другом на \"Animal Джаz\"! Остались очень довольны, атмосфера очень уютная, дружелюбная, есть второй этаж, бар')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь этот текст легко можно разделить на слова, используя пробел в качестве разделителя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построение классификатора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы получить список слов, основы которых входят в список «положительных» наборов слов, достаточно использовать list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'Просто шикарный клуб! Ходили с другом на \"Animal Джаz\"! Остались очень довольны, атмосфера очень уютная, дружелюбная, есть второй этаж, бар'\n",
    "text_no_punctuation = clear_punctuation(text)\n",
    "positive_words_list = [x for x in text_no_punctuation.split(' ') if snowball_stemmer.stem(x) in params['positive']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом тексте оказалось только одно «положительное» слово. Есть чем расширить наши списки для улучшения модели классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['довольны']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words_count = len( [x for x in text_no_punctuation.split(' ') if snowball_stemmer.stem(x) in params['positive']] )\n",
    "positive_words_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оформим эти вычисления в функцию. Если количество положительных отзывов больше, чем отрицательных, возвращаем 'positive'. Если меньше, 'negative'. В случае равенства (в том числе 0) возвращаем 'undef'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier(text):\n",
    "    \"\"\"Классификация отзыва text на 'positive', 'negative' и 'undef' по совпадающим основам слов из params.yaml\"\"\"\n",
    "    text = clear_punctuation(text)\n",
    "    positive_words_count = len( [x for x in text.split(' ') if snowball_stemmer.stem(x) in params['positive']] )\n",
    "    negative_words_count = len( [x for x in text.split(' ') if snowball_stemmer.stem(x) in params['negative']] )\n",
    "    if positive_words_count > negative_words_count:\n",
    "        return 'positive'\n",
    "    elif positive_words_count < negative_words_count:\n",
    "        return 'negative'\n",
    "    return 'undef'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем, как работает (на очевидных примерах):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Просто шикарный клуб! Ходили с другом на \"Animal Джаz\"! Остались очень довольны, атмосфера очень уютная, дружелюбная, есть второй этаж, бар'\n",
    "\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Ужасное место. Сотрудники клуба от них в восторге! А культурным людям тут не место.'\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прогоним наш файл с отзывами через классификатор. В этом скрипте мы используем одни и те же файлы для чтения и записи. В таких случаях рекомендуется «закрывать» их с помощью метода close, чтобы при следующем открытии не возникало проблем, и каждый раз не придумывать новые названия переменных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('texts_opinions.txt', mode = 'r', encoding = 'utf-8')\n",
    "f_classified = open('texts_classified.txt', mode = 'w', encoding = 'utf-8')\n",
    "for line in f:\n",
    "    line = line.strip()    \n",
    "    f_classified.write('{}\\n'.format(classifier(line)))\n",
    "f.close()\n",
    "f_classified.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте сравним оценки нашего классификатора с реальными оценками пользователей в файле texts_ratings.txt. Будем пользовать следующими правилами:\n",
    "\n",
    "Не учитываем отзывы, которые мы не смогли классифицировать (значение 'undef').\n",
    "Для определенных типов отзывов (positive и negative) считаем их общее количество в переменной total_defined_ratings.\n",
    "Если оценка отзыва в файлах совпала, то увеличиваем переменную right_classifications на 1.\n",
    "В конце алгоритма выводим долю верно угаданных отзывов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля верно классифицированных отзывов: 77%\n"
     ]
    }
   ],
   "source": [
    "f_classified = open('texts_classified.txt', mode = 'r', encoding = 'utf-8')\n",
    "f_ratings = open('texts_ratings.txt', mode = 'r', encoding = 'utf-8')\n",
    "classified_list = [line.strip() for line in f_classified]\n",
    "ratings_list = [line.strip() for line in f_ratings]\n",
    "right_classifications = 0\n",
    "total_defined_ratings = 0\n",
    "for i in range(len(classified_list)):  \n",
    "    if classified_list[i] != 'undef':\n",
    "        total_defined_ratings += 1        \n",
    "        if classified_list[i] == ratings_list[i]:\n",
    "            right_classifications += 1\n",
    "print('Доля верно классифицированных отзывов: {:.0%}'.format(right_classifications / total_defined_ratings))\n",
    "f_classified.close()\n",
    "f_ratings.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Доля верно классифицированных отзывов: 77%\n",
    "\n",
    "Итак, при первой самой простой модели мы получили точность классификации 77%. Конечно, это без учета того, что часть отзыва мы не классифицировали. Но долю неопределенных отзывов можно уменьшить, добавив в наш словарь больше «положительных» и «отрицательных» слов.\n",
    "\n",
    "Попробуйте улучшить классификатор, добавив в params.yaml больше слов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
